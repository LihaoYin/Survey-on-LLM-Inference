# Survey-on-LLM-Inference

## Contents

- [0. Introduction](#0-Introduction) <br />

- [1. Paper Reading Progress](#1-Paper-Reading-Progress) <br />

- [2. All Papers (Classifications according to Related Topics)](#2-all-papers-classifications-according-to-related-topics) <br />

- [3. All Paper Survey](#3-all-paper-survey) <br />


# 0. Introduction

## Contributors

## Taxonomy

![Taxonomy of Efficient LLM Inference](https://github.com/LihaoYin/Survey-on-LLM-Inference/blob/main/Images/Taxonomy.png)

# 1. Paper Reading Progress



| Paper Title | Taxonomy | Reader | Link |
| :-------------------------------------------------------------| :-------- | :-------- | :--------|


# 2. All Papers (Classifications according to Taxonomy)
- **Survey/Review**
  
- **大算子**

- [**Quantization**](#33-quantization)

- **Structure Optimization**

- **Knowledge Distillation**

- **Dynamic Inference**

- **System-level Optimization**

- **Retrieval-Augmented Generation**


# 3. All Paper Reading Notes

## 3.1 Survey/Review

## 3.2 大算子

## 3.3 Quantization

## 3.4 Structure Optimization

## 3.5 Knowledge Distillation

## 3.6 Dynamic Inference

### SpecInfer: Accelerating Large Language Model Serving with Tree-based Speculative Inference and Verification (ASPLOS 2024)

* <img src="Image/pdf_24px.png">[Paper](https://dl.acm.org/doi/abs/10.1145/3620666.3651335)
  
**Notes:** 

## 3.7 System-level Optimization

## 3.8 Retrieval-Augmented Generation
